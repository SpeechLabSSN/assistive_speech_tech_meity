# python3 -m espnet2.bin.tts_train --collect_stats true --write_collected_feats false --use_preprocessor true --token_type phn --token_list dump/token_list/phn_espeak_ng_tamil/tokens.txt --non_linguistic_symbols none --cleaner none --g2p espeak_ng_tamil --normalize none --pitch_normalize none --energy_normalize none --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/valid/text,text,text --valid_data_path_and_name_and_type dump/raw/valid/wav.scp,speech,sound --train_shape_file exp/tts_stats_raw_phn_espeak_ng_tamil/logdir/train.26.scp --valid_shape_file exp/tts_stats_raw_phn_espeak_ng_tamil/logdir/valid.26.scp --output_dir exp/tts_stats_raw_phn_espeak_ng_tamil/logdir/stats.26 --config conf/train.yaml --feats_extract fbank --feats_extract_conf n_fft=1024 --feats_extract_conf hop_length=256 --feats_extract_conf win_length=null --feats_extract_conf fs=16000 --feats_extract_conf fmin=80 --feats_extract_conf fmax=7600 --feats_extract_conf n_mels=80 --pitch_extract_conf fs=16000 --pitch_extract_conf n_fft=1024 --pitch_extract_conf hop_length=256 --pitch_extract_conf f0max=400 --pitch_extract_conf f0min=80 --energy_extract_conf fs=16000 --energy_extract_conf n_fft=1024 --energy_extract_conf hop_length=256 --energy_extract_conf win_length=null --train_data_path_and_name_and_type dump/xvector/train/xvector.scp,spembs,kaldi_ark --valid_data_path_and_name_and_type dump/xvector/valid/xvector.scp,spembs,kaldi_ark 
# Started at Thu Feb 15 09:30:14 +0530 2024
#
/home/speechlab/espnet/tools/espnet_venv/bin/python3 /home/speechlab/espnet/espnet2/bin/tts_train.py --collect_stats true --write_collected_feats false --use_preprocessor true --token_type phn --token_list dump/token_list/phn_espeak_ng_tamil/tokens.txt --non_linguistic_symbols none --cleaner none --g2p espeak_ng_tamil --normalize none --pitch_normalize none --energy_normalize none --train_data_path_and_name_and_type dump/raw/train/text,text,text --train_data_path_and_name_and_type dump/raw/train/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/valid/text,text,text --valid_data_path_and_name_and_type dump/raw/valid/wav.scp,speech,sound --train_shape_file exp/tts_stats_raw_phn_espeak_ng_tamil/logdir/train.26.scp --valid_shape_file exp/tts_stats_raw_phn_espeak_ng_tamil/logdir/valid.26.scp --output_dir exp/tts_stats_raw_phn_espeak_ng_tamil/logdir/stats.26 --config conf/train.yaml --feats_extract fbank --feats_extract_conf n_fft=1024 --feats_extract_conf hop_length=256 --feats_extract_conf win_length=null --feats_extract_conf fs=16000 --feats_extract_conf fmin=80 --feats_extract_conf fmax=7600 --feats_extract_conf n_mels=80 --pitch_extract_conf fs=16000 --pitch_extract_conf n_fft=1024 --pitch_extract_conf hop_length=256 --pitch_extract_conf f0max=400 --pitch_extract_conf f0min=80 --energy_extract_conf fs=16000 --energy_extract_conf n_fft=1024 --energy_extract_conf hop_length=256 --energy_extract_conf win_length=null --train_data_path_and_name_and_type dump/xvector/train/xvector.scp,spembs,kaldi_ark --valid_data_path_and_name_and_type dump/xvector/valid/xvector.scp,spembs,kaldi_ark
[speechlab-HP-Z6-G5-Workstation-Desktop-PC] 2024-02-15 09:30:18,740 (tts:291) INFO: Vocabulary size: 107
[speechlab-HP-Z6-G5-Workstation-Desktop-PC] 2024-02-15 09:30:19,229 (abs_task:1271) INFO: pytorch.version=2.1.0+cu121, cuda.available=True, cudnn.version=8902, cudnn.benchmark=False, cudnn.deterministic=True
[speechlab-HP-Z6-G5-Workstation-Desktop-PC] 2024-02-15 09:30:19,230 (abs_task:1272) INFO: Model structure:
ESPnetTTSModel(
  (feats_extract): LogMelFbank(
    (stft): Stft(n_fft=1024, win_length=1024, hop_length=256, center=True, normalized=False, onesided=True)
    (logmel): LogMel(sr=16000, n_fft=1024, n_mels=80, fmin=80, fmax=7600, htk=False)
  )
  (tts): Tacotron2(
    (enc): Encoder(
      (embed): Embedding(107, 512, padding_idx=0)
      (convs): ModuleList(
        (0-2): 3 x Sequential(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)
          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Dropout(p=0.5, inplace=False)
        )
      )
      (blstm): LSTM(512, 256, batch_first=True, bidirectional=True)
    )
    (projection): Linear(in_features=512, out_features=512, bias=True)
    (dec): Decoder(
      (att): AttLoc(
        (mlp_enc): Linear(in_features=512, out_features=512, bias=True)
        (mlp_dec): Linear(in_features=1024, out_features=512, bias=False)
        (mlp_att): Linear(in_features=32, out_features=512, bias=False)
        (loc_conv): Conv2d(1, 32, kernel_size=(1, 31), stride=(1, 1), padding=(0, 15), bias=False)
        (gvec): Linear(in_features=512, out_features=1, bias=True)
      )
      (lstm): ModuleList(
        (0): ZoneOutCell(
          (cell): LSTMCell(768, 1024)
        )
        (1): ZoneOutCell(
          (cell): LSTMCell(1024, 1024)
        )
      )
      (prenet): Prenet(
        (prenet): ModuleList(
          (0): Sequential(
            (0): Linear(in_features=80, out_features=256, bias=True)
            (1): ReLU()
          )
          (1): Sequential(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): ReLU()
          )
        )
      )
      (postnet): Postnet(
        (postnet): ModuleList(
          (0): Sequential(
            (0): Conv1d(80, 512, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)
            (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Tanh()
            (3): Dropout(p=0.5, inplace=False)
          )
          (1-3): 3 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)
            (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Tanh()
            (3): Dropout(p=0.5, inplace=False)
          )
          (4): Sequential(
            (0): Conv1d(512, 80, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)
            (1): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (feat_out): Linear(in_features=1536, out_features=80, bias=False)
      (prob_out): Linear(in_features=1536, out_features=1, bias=True)
    )
    (taco2_loss): Tacotron2Loss(
      (l1_criterion): L1Loss()
      (mse_criterion): MSELoss()
      (bce_criterion): BCEWithLogitsLoss()
    )
    (attn_loss): GuidedAttentionLoss()
  )
)

Model summary:
    Class Name: ESPnetTTSModel
    Total Number of model parameters: 26.94 M
    Number of trainable parameters: 26.94 M (100.0%)
    Size: 107.75 MB
    Type: torch.float32
[speechlab-HP-Z6-G5-Workstation-Desktop-PC] 2024-02-15 09:30:19,230 (abs_task:1275) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-06
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
[speechlab-HP-Z6-G5-Workstation-Desktop-PC] 2024-02-15 09:30:19,230 (abs_task:1276) INFO: Scheduler: None
[speechlab-HP-Z6-G5-Workstation-Desktop-PC] 2024-02-15 09:30:19,230 (abs_task:1285) INFO: Saving the configuration in exp/tts_stats_raw_phn_espeak_ng_tamil/logdir/stats.26/config.yaml
[speechlab-HP-Z6-G5-Workstation-Desktop-PC] 2024-02-15 09:30:19,237 (abs_task:1296) INFO: Namespace(config='conf/train.yaml', print_config=False, log_level='INFO', drop_last_iter=False, dry_run=False, iterator_type='sequence', valid_iterator_type=None, output_dir='exp/tts_stats_raw_phn_espeak_ng_tamil/logdir/stats.26', ngpu=0, seed=0, num_workers=1, num_att_plot=3, dist_backend='nccl', dist_init_method='env://', dist_world_size=None, dist_rank=None, local_rank=None, dist_master_addr=None, dist_master_port=None, dist_launcher=None, multiprocessing_distributed=False, unused_parameters=False, sharded_ddp=False, cudnn_enabled=True, cudnn_benchmark=False, cudnn_deterministic=True, collect_stats=True, write_collected_feats=False, max_epoch=200, patience=None, val_scheduler_criterion=('valid', 'loss'), early_stopping_criterion=('valid', 'loss', 'min'), best_model_criterion=[['valid', 'loss', 'min'], ['train', 'loss', 'min']], keep_nbest_models=5, nbest_averaging_interval=0, grad_clip=1.0, grad_clip_type=2.0, grad_noise=False, accum_grad=1, no_forward_run=False, resume=False, train_dtype='float32', use_amp=False, log_interval=None, use_matplotlib=True, use_tensorboard=True, create_graph_in_tensorboard=False, use_wandb=False, wandb_project=None, wandb_id=None, wandb_entity=None, wandb_name=None, wandb_model_log_interval=-1, detect_anomaly=False, use_lora=False, save_lora_only=True, lora_conf={}, pretrain_path=None, init_param=[], ignore_init_mismatch=False, freeze_param=[], num_iters_per_epoch=500, batch_size=20, valid_batch_size=None, batch_bins=3750000, valid_batch_bins=None, train_shape_file=['exp/tts_stats_raw_phn_espeak_ng_tamil/logdir/train.26.scp'], valid_shape_file=['exp/tts_stats_raw_phn_espeak_ng_tamil/logdir/valid.26.scp'], batch_type='numel', valid_batch_type=None, fold_length=[], sort_in_batch='descending', shuffle_within_batch=False, sort_batch='descending', multiple_iterator=False, chunk_length=500, chunk_shift_ratio=0.5, num_cache_chunks=1024, chunk_excluded_key_prefixes=[], chunk_default_fs=None, train_data_path_and_name_and_type=[('dump/raw/train/text', 'text', 'text'), ('dump/raw/train/wav.scp', 'speech', 'sound'), ('dump/xvector/train/xvector.scp', 'spembs', 'kaldi_ark')], valid_data_path_and_name_and_type=[('dump/raw/valid/text', 'text', 'text'), ('dump/raw/valid/wav.scp', 'speech', 'sound'), ('dump/xvector/valid/xvector.scp', 'spembs', 'kaldi_ark')], allow_variable_data_keys=False, max_cache_size=0.0, max_cache_fd=32, allow_multi_rates=False, valid_max_cache_size=None, exclude_weight_decay=False, exclude_weight_decay_conf={}, optim='adam', optim_conf={'lr': 0.001, 'eps': 1e-06, 'weight_decay': 0.0}, scheduler=None, scheduler_conf={}, token_list=['<blank>', '<unk>', 'ʌ', 'ʉ', 'n', 'm', 'k', 'i', 'd', 'p', 't', 'l', 'ˈʌ', 'r', 'v', 'ɹ', '.', 'j', 'ɡ', 'ɖ', 'aː', 'aɪ', 'ʈ', 'ˌʌ', 'ˈaː', 'ɭ', 'ˈi', 's', 'ˈe', 'ˈu', 'ɳ', 'ˌi', 'b', 'ˈa', 'ɻ', 'ˈo', 'ŋ', 'ˈeː', 'eː', 'ˌaː', 'ˌʉ', 'ʲ', 'ˈoː', 'ˈuː', 'dʒ', 'ˈiː', 'ʂ', 'tʃː', ',ˈa', 'oː', ',ʲ', 'ˌaɪ', 'ɲ', ',p', ',ˈi', ',k', ',s', 'iː', ',n', 'ˈaɪ', ',m', ',t', ',v', 'e', 'h', 'tʃ', 'o', 'ˌeː', ',ˈu', ',ˈaː', 'ˌoː', 'ˌe', ',ˈo', 'ˌiː', ',ɹ', ',b', 'f', 'ˌo', ',j', ',dʒ', ',tʃ', ',d', ',ɡ', ',h', ',l', 'uː', 'z', ',ˈaɪ', ',ˈoː', ',ɖ', 'ˈaʊ', 'ˌuː', ',ˈiː', ',ˈuː', ',f', ',ʂ', 'ˈʉ', ',ʈ', ',ɲ', 'w', 'aʊ', ',r', 'ˌaʊ', ',.', ',ɻ', ',z', '<sos/eos>'], odim=None, model_conf={}, use_preprocessor=True, token_type='phn', bpemodel=None, non_linguistic_symbols=None, cleaner=None, g2p='espeak_ng_tamil', feats_extract='fbank', feats_extract_conf={'n_fft': 1024, 'hop_length': 256, 'win_length': None, 'fs': 16000, 'fmin': 80, 'fmax': 7600, 'n_mels': 80}, normalize=None, normalize_conf={}, tts='tacotron2', tts_conf={'embed_dim': 512, 'elayers': 1, 'eunits': 512, 'econv_layers': 3, 'econv_chans': 512, 'econv_filts': 5, 'atype': 'location', 'adim': 512, 'aconv_chans': 32, 'aconv_filts': 15, 'cumulate_att_w': True, 'dlayers': 2, 'dunits': 1024, 'prenet_layers': 2, 'prenet_units': 256, 'postnet_layers': 5, 'postnet_chans': 512, 'postnet_filts': 5, 'output_activation': None, 'use_batch_norm': True, 'use_concate': True, 'use_residual': False, 'dropout_rate': 0.5, 'zoneout_rate': 0.1, 'reduction_factor': 1, 'spk_embed_dim': 512, 'spk_embed_integration_type': 'add', 'use_masking': True, 'bce_pos_weight': 5.0, 'use_guided_attn_loss': True, 'guided_attn_loss_sigma': 0.4, 'guided_attn_loss_lambda': 1.0}, pitch_extract=None, pitch_extract_conf={'fs': 16000, 'n_fft': 1024, 'hop_length': 256, 'f0max': 400, 'f0min': 80}, pitch_normalize=None, pitch_normalize_conf={}, energy_extract=None, energy_extract_conf={'fs': 16000, 'n_fft': 1024, 'hop_length': 256, 'win_length': None}, energy_normalize=None, energy_normalize_conf={}, required=['output_dir', 'token_list'], version='202402', distributed=False)
# Accounting: time=16 threads=1
# Ended (code 0) at Thu Feb 15 09:30:30 +0530 2024, elapsed time 16 seconds
